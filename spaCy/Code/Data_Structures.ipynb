{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Value - 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hash Value - {nlp.vocab.strings[\"coffee\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Value - coffee\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hash Value - {nlp.vocab.strings[3197928453018144401]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc -> Linguistic attributes (dependency , pos)\n",
    "# Vocab -> Lexical attribute (IS_DIGIT , IS_ALPHA , TEXT)\n",
    "# Vocab (knows Hash ID of every string) -> To get string -> Look-up table [string:hash-id] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5439657043933447811"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = (\"I have a cat\")\n",
    "\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "cat_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[5439657043933447811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc , Span\n",
    "\n",
    "nlp = spacy.blank(\"en\") # initiating nlp object \n",
    "\n",
    "words = [\"hello\" , \"world\" , \"!\"]\n",
    "spaces = [True , False , False]\n",
    "\n",
    "doc = Doc(nlp.vocab , words = words , spaces = spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hello world!"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Span(doc , 1 , 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_with_label = Span(doc , 0 ,2 ,  label = \"GREETING\")\n",
    "doc.ents = [span_with_label] # doc.ents is writable , so as per our task we can add entities manually if not in trained data/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREETING hello world\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.label_ , ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON David Bowie\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc , Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "doc = Doc(nlp.vocab , words = words , spaces = spaces)\n",
    "\n",
    "span_name_label = Span(doc , 2 , 4 , label = \"PERSON\")\n",
    "\n",
    "print(span_name_label.label_ , span_name_label.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = [span_name_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(David Bowie,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(doc.ents) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "print(doc[0].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin looks\n"
     ]
    }
   ],
   "source": [
    "# The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb. (in the most optimized way)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        index = token.i + 1\n",
    "        next_token = doc[index]\n",
    "        if next_token.pos_ == \"VERB\":\n",
    "            print(token.text , next_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8382381200790405\n"
     ]
    }
   ],
   "source": [
    "# To compare similarity , use -> medium and large models , they only have vectors for comparing similarity\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp (\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Comparing two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8382381200790405\n"
     ]
    }
   ],
   "source": [
    "print(doc.similarity(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token_1 = doc[2]\n",
    "token_2 = doc[4]\n",
    "print(token_1.similarity(token_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1310572326183319\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Indian Institute of Technology\")\n",
    "token = nlp(\"collge\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "# comparing a token with a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7478198409080505\n"
     ]
    }
   ],
   "source": [
    "span = nlp(\"Developed an app which works end to end\")[0:5]\n",
    "doc = nlp(\"Work experience is essential\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc and Span vectors default to average of token vectors\n",
    "\n",
    "doc = nlp(\"India is a country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(doc[-1].vector)) # 300 size vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.75987482e-01  2.95220017e-01  5.53175062e-02  7.55575001e-02\n",
      "  2.32170239e-01  2.18925253e-03  3.40092510e-01 -1.78802505e-01\n",
      "  5.36397509e-02  2.01376987e+00 -4.94089961e-01 -3.38320017e-01\n",
      " -2.07768500e-01 -8.56000185e-03 -3.16640027e-02 -1.35674998e-02\n",
      " -2.19487503e-01  1.29222238e+00  7.07562789e-02  1.13817498e-01\n",
      " -2.82294989e-01 -2.52393246e-01  1.69265002e-01 -2.64062762e-01\n",
      " -7.14329928e-02 -7.96277449e-02  2.72385001e-01  8.87579992e-02\n",
      " -2.39240006e-01 -8.01199824e-02  8.09949934e-02 -4.08015013e-01\n",
      " -7.13434964e-02 -1.95002496e-01 -3.27162504e-01  8.67996365e-04\n",
      " -1.15750000e-01 -2.64487505e-01 -2.05805004e-01 -6.94500059e-02\n",
      "  2.80475020e-01 -2.13334009e-01  1.52083606e-01 -1.83649004e-01\n",
      "  2.16212999e-02 -3.12228501e-01  2.55757514e-02  1.40500069e-02\n",
      "  1.02404952e-02  7.96445012e-02 -2.58964747e-01  4.23509002e-01\n",
      " -9.63289961e-02 -2.66611516e-01  2.10869998e-01  1.01421498e-01\n",
      "  1.41084492e-01 -1.27255023e-02  1.47844747e-01 -1.27790004e-01\n",
      " -1.93052500e-01 -2.81198233e-01 -4.56644967e-02 -2.72642493e-01\n",
      " -1.62919983e-02 -3.12528014e-01 -3.68978754e-02  1.32257000e-01\n",
      "  2.64634997e-01  2.78687514e-02  1.10249966e-03  2.05671743e-01\n",
      " -6.71204999e-02 -1.48982510e-01  3.20805252e-01  2.38587499e-01\n",
      " -1.01676494e-01 -1.22520499e-01  1.26912490e-01  1.96938008e-01\n",
      "  9.49399918e-02  1.10705256e-01 -4.08494994e-02  1.79572508e-01\n",
      "  5.41724935e-02 -3.25500757e-01 -1.95625089e-02  1.19532496e-01\n",
      "  2.11649910e-02 -2.57152498e-01 -2.15721756e-01  2.67042488e-01\n",
      " -4.00437504e-01 -7.85749406e-03 -7.92220011e-02 -4.62257490e-02\n",
      "  1.95677519e-01 -6.57125115e-02  3.62050533e-03 -9.04764980e-02\n",
      "  1.88923016e-01  1.34052500e-01  1.13124996e-02 -9.06650349e-03\n",
      "  1.79617509e-01 -1.65240002e+00 -4.48935032e-01 -3.26449946e-02\n",
      " -3.08349878e-02 -7.79410005e-02 -3.59325036e-02 -2.11974993e-01\n",
      " -5.58549985e-02 -4.09612477e-01 -2.62184978e-01 -2.99117506e-01\n",
      "  3.64750177e-02 -9.73800048e-02 -1.22490004e-02 -7.57225007e-02\n",
      " -2.71433055e-01  1.89192995e-01  6.39199987e-02 -3.43997508e-01\n",
      " -1.50095254e-01  2.80372500e-01  2.32833475e-01 -2.35532492e-01\n",
      "  9.29260030e-02 -8.80964994e-02  2.98552513e-01 -2.81991512e-01\n",
      " -1.65242493e-01  4.24920022e-02  1.25739500e-01 -5.12549281e-03\n",
      " -2.03819498e-01 -1.35329992e-01 -8.17649961e-02  2.54760012e-02\n",
      " -8.93771768e-01  1.25825018e-01  4.44112509e-01  3.46079990e-02\n",
      "  1.02400251e-01 -3.24629992e-01  1.70825005e-01  1.84835002e-01\n",
      "  7.30874911e-02  6.33810014e-02  4.34470028e-02  2.42447525e-01\n",
      "  3.44697535e-02 -8.57449919e-02  3.71800028e-02  2.93315738e-01\n",
      "  4.48650010e-02  2.16115564e-01 -1.58755004e-01 -1.81341499e-01\n",
      "  1.81229979e-01 -6.80922568e-02  3.86503756e-01  1.37736499e-01\n",
      " -3.83168235e-02 -5.89199997e-02  6.84910044e-02 -3.38348985e-01\n",
      " -8.10105056e-02 -1.38437003e-01 -2.13488996e-01 -2.02446014e-01\n",
      "  1.40141502e-01  2.34649241e-01  4.08152454e-02 -1.68088496e-01\n",
      " -8.97947550e-02  7.51539990e-02  2.67565489e-01  1.67732507e-01\n",
      " -1.42699480e-03  1.51689291e-01 -1.87649988e-02  2.93095000e-02\n",
      "  3.86889935e-01 -1.92672521e-01 -1.34585053e-02 -2.50436485e-01\n",
      "  2.46351555e-01  1.62459999e-01 -2.03600004e-02 -9.39665064e-02\n",
      " -4.41525020e-02 -1.16077505e-01 -3.87849987e-01 -2.25007758e-01\n",
      "  2.75528252e-01  1.08593516e-01 -1.60699248e-01 -1.98071003e-01\n",
      "  2.00358495e-01 -1.84067249e-01 -6.24639988e-02  1.45899013e-01\n",
      " -2.39399999e-01 -1.24054998e-01 -7.50724971e-02 -9.10369977e-02\n",
      " -9.44380015e-02  6.64780438e-02 -8.59514922e-02 -8.65204930e-02\n",
      " -1.91647485e-02 -1.58122510e-01  9.88632515e-02 -9.69099924e-02\n",
      "  4.54699919e-02  1.47644997e-01 -7.80169964e-02 -1.13543987e-01\n",
      " -2.23101512e-01  9.42125022e-02 -1.55194923e-02  1.47996014e-02\n",
      "  1.45485029e-02 -5.89145049e-02 -3.06597710e-01 -2.30715051e-02\n",
      " -2.71799982e-01  3.46979946e-02  2.14832738e-01 -9.70537588e-02\n",
      " -5.90550005e-02 -3.83134969e-02  5.19500002e-02 -2.34439999e-01\n",
      " -8.56954977e-02 -1.60929248e-01  1.59550086e-03  1.82524979e-01\n",
      " -7.09500089e-02  8.98312479e-02 -1.23121999e-01 -2.15775073e-02\n",
      "  3.07119995e-01 -2.83337533e-02 -8.18424895e-02 -1.57450512e-01\n",
      " -8.51924717e-03  3.42649929e-02  8.55749846e-03  1.80030257e-01\n",
      " -5.48015013e-02 -2.18216747e-01  6.26065135e-02  1.47073492e-01\n",
      "  2.78840005e-01  8.32199901e-02 -2.32100517e-01  1.68699995e-01\n",
      "  1.41712487e-01  2.53924996e-01 -5.92250377e-04  1.39687508e-01\n",
      " -1.07103750e-01 -2.76889980e-01 -8.68095011e-02 -6.02104999e-02\n",
      "  2.84242511e-01  5.45782506e-01 -2.15492740e-01 -1.16557501e-01\n",
      " -4.47834969e-01 -6.90400153e-02 -1.87714994e-01  6.97975010e-02\n",
      "  1.64787501e-01  4.95448709e-03 -1.12649500e-01  4.89387512e-01\n",
      "  2.12379992e-01  1.76667497e-01  1.67250000e-02  2.29433492e-01\n",
      " -4.01185043e-02 -6.04725033e-02  2.16141492e-01 -3.49277496e-01\n",
      "  1.32349998e-01 -1.32819995e-01 -1.34097755e-01  1.95312500e-01\n",
      "  7.61125013e-02  1.45067498e-01  9.03587490e-02  4.00131762e-01\n",
      "  1.48030743e-01 -9.06717628e-02 -3.70075032e-02 -4.61975038e-02]\n"
     ]
    }
   ],
   "source": [
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = []\n",
    "\n",
    "for token in doc:\n",
    "    doc_vector.append(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/pj9qbj8x6dn49165fn9tyd7m0000gn/T/ipykernel_1446/1241076048.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  tensor = torch.tensor(doc_vector)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8955,  0.3877,  0.6498,  ...,  0.1375, -0.2671,  0.5757],\n",
       "        [-0.6005,  0.1884, -0.4099,  ..., -0.2780,  0.3123, -0.2833],\n",
       "        [-0.6005,  0.1884, -0.4099,  ..., -0.2780,  0.3123, -0.2833],\n",
       "        [-0.6074,  0.4164,  0.3913,  ...,  0.0558, -0.5056, -0.1939]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_average = tensor.float().mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.7599e-01,  2.9522e-01,  5.5318e-02,  7.5558e-02,  2.3217e-01,\n",
       "         2.1893e-03,  3.4009e-01, -1.7880e-01,  5.3640e-02,  2.0138e+00,\n",
       "        -4.9409e-01, -3.3832e-01, -2.0777e-01, -8.5600e-03, -3.1664e-02,\n",
       "        -1.3567e-02, -2.1949e-01,  1.2922e+00,  7.0756e-02,  1.1382e-01,\n",
       "        -2.8229e-01, -2.5239e-01,  1.6927e-01, -2.6406e-01, -7.1433e-02,\n",
       "        -7.9628e-02,  2.7239e-01,  8.8758e-02, -2.3924e-01, -8.0120e-02,\n",
       "         8.0995e-02, -4.0802e-01, -7.1343e-02, -1.9500e-01, -3.2716e-01,\n",
       "         8.6800e-04, -1.1575e-01, -2.6449e-01, -2.0581e-01, -6.9450e-02,\n",
       "         2.8048e-01, -2.1333e-01,  1.5208e-01, -1.8365e-01,  2.1621e-02,\n",
       "        -3.1223e-01,  2.5576e-02,  1.4050e-02,  1.0240e-02,  7.9645e-02,\n",
       "        -2.5896e-01,  4.2351e-01, -9.6329e-02, -2.6661e-01,  2.1087e-01,\n",
       "         1.0142e-01,  1.4108e-01, -1.2726e-02,  1.4784e-01, -1.2779e-01,\n",
       "        -1.9305e-01, -2.8120e-01, -4.5664e-02, -2.7264e-01, -1.6292e-02,\n",
       "        -3.1253e-01, -3.6898e-02,  1.3226e-01,  2.6463e-01,  2.7869e-02,\n",
       "         1.1025e-03,  2.0567e-01, -6.7120e-02, -1.4898e-01,  3.2081e-01,\n",
       "         2.3859e-01, -1.0168e-01, -1.2252e-01,  1.2691e-01,  1.9694e-01,\n",
       "         9.4940e-02,  1.1071e-01, -4.0849e-02,  1.7957e-01,  5.4172e-02,\n",
       "        -3.2550e-01, -1.9563e-02,  1.1953e-01,  2.1165e-02, -2.5715e-01,\n",
       "        -2.1572e-01,  2.6704e-01, -4.0044e-01, -7.8575e-03, -7.9222e-02,\n",
       "        -4.6226e-02,  1.9568e-01, -6.5713e-02,  3.6205e-03, -9.0476e-02,\n",
       "         1.8892e-01,  1.3405e-01,  1.1312e-02, -9.0665e-03,  1.7962e-01,\n",
       "        -1.6524e+00, -4.4894e-01, -3.2645e-02, -3.0835e-02, -7.7941e-02,\n",
       "        -3.5933e-02, -2.1197e-01, -5.5855e-02, -4.0961e-01, -2.6218e-01,\n",
       "        -2.9912e-01,  3.6475e-02, -9.7380e-02, -1.2249e-02, -7.5723e-02,\n",
       "        -2.7143e-01,  1.8919e-01,  6.3920e-02, -3.4400e-01, -1.5010e-01,\n",
       "         2.8037e-01,  2.3283e-01, -2.3553e-01,  9.2926e-02, -8.8096e-02,\n",
       "         2.9855e-01, -2.8199e-01, -1.6524e-01,  4.2492e-02,  1.2574e-01,\n",
       "        -5.1255e-03, -2.0382e-01, -1.3533e-01, -8.1765e-02,  2.5476e-02,\n",
       "        -8.9377e-01,  1.2583e-01,  4.4411e-01,  3.4608e-02,  1.0240e-01,\n",
       "        -3.2463e-01,  1.7083e-01,  1.8484e-01,  7.3087e-02,  6.3381e-02,\n",
       "         4.3447e-02,  2.4245e-01,  3.4470e-02, -8.5745e-02,  3.7180e-02,\n",
       "         2.9332e-01,  4.4865e-02,  2.1612e-01, -1.5876e-01, -1.8134e-01,\n",
       "         1.8123e-01, -6.8092e-02,  3.8650e-01,  1.3774e-01, -3.8317e-02,\n",
       "        -5.8920e-02,  6.8491e-02, -3.3835e-01, -8.1011e-02, -1.3844e-01,\n",
       "        -2.1349e-01, -2.0245e-01,  1.4014e-01,  2.3465e-01,  4.0815e-02,\n",
       "        -1.6809e-01, -8.9795e-02,  7.5154e-02,  2.6757e-01,  1.6773e-01,\n",
       "        -1.4270e-03,  1.5169e-01, -1.8765e-02,  2.9310e-02,  3.8689e-01,\n",
       "        -1.9267e-01, -1.3459e-02, -2.5044e-01,  2.4635e-01,  1.6246e-01,\n",
       "        -2.0360e-02, -9.3967e-02, -4.4153e-02, -1.1608e-01, -3.8785e-01,\n",
       "        -2.2501e-01,  2.7553e-01,  1.0859e-01, -1.6070e-01, -1.9807e-01,\n",
       "         2.0036e-01, -1.8407e-01, -6.2464e-02,  1.4590e-01, -2.3940e-01,\n",
       "        -1.2405e-01, -7.5072e-02, -9.1037e-02, -9.4438e-02,  6.6478e-02,\n",
       "        -8.5951e-02, -8.6520e-02, -1.9165e-02, -1.5812e-01,  9.8863e-02,\n",
       "        -9.6910e-02,  4.5470e-02,  1.4764e-01, -7.8017e-02, -1.1354e-01,\n",
       "        -2.2310e-01,  9.4213e-02, -1.5519e-02,  1.4800e-02,  1.4549e-02,\n",
       "        -5.8915e-02, -3.0660e-01, -2.3072e-02, -2.7180e-01,  3.4698e-02,\n",
       "         2.1483e-01, -9.7054e-02, -5.9055e-02, -3.8313e-02,  5.1950e-02,\n",
       "        -2.3444e-01, -8.5695e-02, -1.6093e-01,  1.5955e-03,  1.8252e-01,\n",
       "        -7.0950e-02,  8.9831e-02, -1.2312e-01, -2.1578e-02,  3.0712e-01,\n",
       "        -2.8334e-02, -8.1842e-02, -1.5745e-01, -8.5192e-03,  3.4265e-02,\n",
       "         8.5575e-03,  1.8003e-01, -5.4802e-02, -2.1822e-01,  6.2607e-02,\n",
       "         1.4707e-01,  2.7884e-01,  8.3220e-02, -2.3210e-01,  1.6870e-01,\n",
       "         1.4171e-01,  2.5392e-01, -5.9225e-04,  1.3969e-01, -1.0710e-01,\n",
       "        -2.7689e-01, -8.6810e-02, -6.0210e-02,  2.8424e-01,  5.4578e-01,\n",
       "        -2.1549e-01, -1.1656e-01, -4.4783e-01, -6.9040e-02, -1.8771e-01,\n",
       "         6.9798e-02,  1.6479e-01,  4.9545e-03, -1.1265e-01,  4.8939e-01,\n",
       "         2.1238e-01,  1.7667e-01,  1.6725e-02,  2.2943e-01, -4.0119e-02,\n",
       "        -6.0473e-02,  2.1614e-01, -3.4928e-01,  1.3235e-01, -1.3282e-01,\n",
       "        -1.3410e-01,  1.9531e-01,  7.6113e-02,  1.4507e-01,  9.0359e-02,\n",
       "         4.0013e-01,  1.4803e-01, -9.0672e-02, -3.7008e-02, -4.6198e-02])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_conversion = torch.tensor(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool = True\n",
    "for i,j in zip(tensor_conversion,column_average):\n",
    "    if i != j :\n",
    "        bool = False\n",
    "\n",
    "bool        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the power of Machine learning Models along with rule based methods is very powerful thing\n",
    "# span is segment of doc , is same data structure as doc , so span have all lingustic attributes\n",
    "# If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. Span objects give us access to the original document and all other token attributes and linguistic features predicted by a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase matcher -> Regular expression + token access\n",
    "# Use of token access ? -> can get lingustic attributes of token and also get left and right span of the extracted word match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "pattern = nlp(\"Golden Retriever\") # instead of list of dictionaries , in this we give doc object\n",
    "matcher.add(\"DOG\" , [pattern])\n",
    "\n",
    "doc = nlp(\"I have a dog of breed Golden Retriever\")\n",
    "\n",
    "for match_id , start , end in matcher(doc):\n",
    "    span = doc[start:end] # -> access of the token\n",
    "    print(\"Matched span\" , span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "# Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "# Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun.\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "temp = []\n",
    "\n",
    "for token in doc:\n",
    "    temp.append(token)\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"Amazon\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Twitch,\n",
       " Prime,\n",
       " ,,\n",
       " the,\n",
       " perks,\n",
       " program,\n",
       " for,\n",
       " Amazon,\n",
       " Prime,\n",
       " members,\n",
       " offering,\n",
       " free,\n",
       " loot,\n",
       " ,,\n",
       " games,\n",
       " and,\n",
       " other,\n",
       " benefits,\n",
       " ,,\n",
       " is,\n",
       " ditching,\n",
       " one,\n",
       " of,\n",
       " its,\n",
       " best,\n",
       " features,\n",
       " :,\n",
       " ad,\n",
       " -,\n",
       " free,\n",
       " viewing,\n",
       " .,\n",
       " According,\n",
       " to,\n",
       " an,\n",
       " email,\n",
       " sent,\n",
       " out,\n",
       " to,\n",
       " Amazon,\n",
       " Prime,\n",
       " members,\n",
       " today,\n",
       " ,,\n",
       " ad,\n",
       " -,\n",
       " free,\n",
       " viewing,\n",
       " will,\n",
       " no,\n",
       " longer,\n",
       " be,\n",
       " included,\n",
       " as,\n",
       " a,\n",
       " part,\n",
       " of,\n",
       " Twitch,\n",
       " Prime,\n",
       " for,\n",
       " new,\n",
       " members,\n",
       " ,,\n",
       " beginning,\n",
       " on,\n",
       " September,\n",
       " 14,\n",
       " .,\n",
       " However,\n",
       " ,,\n",
       " members,\n",
       " with,\n",
       " existing,\n",
       " annual,\n",
       " subscriptions,\n",
       " will,\n",
       " be,\n",
       " able,\n",
       " to,\n",
       " continue,\n",
       " to,\n",
       " enjoy,\n",
       " ad,\n",
       " -,\n",
       " free,\n",
       " viewing,\n",
       " until,\n",
       " their,\n",
       " subscription,\n",
       " comes,\n",
       " up,\n",
       " for,\n",
       " renewal,\n",
       " .,\n",
       " Those,\n",
       " with,\n",
       " monthly,\n",
       " subscriptions,\n",
       " will,\n",
       " have,\n",
       " access,\n",
       " to,\n",
       " ad,\n",
       " -,\n",
       " free,\n",
       " viewing,\n",
       " until,\n",
       " October,\n",
       " 15,\n",
       " .]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env-3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
