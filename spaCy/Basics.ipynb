{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "# passing it through nlp object made from spacy.blank() in english will tokenize it and we can access tokens as per our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "print(doc[1].text) # Indexing direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  :  [0, 1, 2, 3, 4]\n",
      "Text  :  ['It', 'costs', '$', '598,76.22', '.']\n",
      "is_alpha  :  [True, True, False, False, False]\n",
      "is_punct  :  [False, False, False, False, True]\n",
      "like_num  :  [False, False, False, True, False]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It costs $598,76.22 .\")\n",
    "\n",
    "# Lexical Attributes , doesn't depend on token's context\n",
    "\n",
    "print(f\"Index  :  {[token.i for token in doc]}\") #indexing very-powerful\n",
    "print(f\"Text  :  {[token.text for token in doc]}\")\n",
    "print(f\"is_alpha  :  {[token.is_alpha for token in doc]}\")\n",
    "print(f\"is_punct  :  {[token.is_punct for token in doc]}\")\n",
    "print(f\"like_num  :  {[token.like_num for token in doc]}\") # like num would also be true for 10 , TEN , ONE ZERO not necessary a number only.\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for two subsequent tokens: a number and a percent sign.\n",
    "\n",
    "# Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
    "# Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n",
    "# Check whether the next token’s text attribute is a percent sign ”%“.\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "        \"Now less than 4% are.\"  )\n",
    "\n",
    "for i in range(0,len(doc)):\n",
    "    if doc[i].like_num:\n",
    "        if doc[i+1] == \"%\":\n",
    "            print(doc[i],doc[i+1])\n",
    "\n",
    "# This code does not work correctly , although logic may seem correct\n",
    "# NOT AN ADVISABLE APPROACH , use token.text method to compare strings with strings , otherwise - will not get the desired result            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60%\n",
      "4%\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "        \"Now less than 4% are.\"  )\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(f\"{token.text}{doc[token.i + 1]}\") \n",
    "\n",
    "# use in-built index in tokens from nlp object itself and have to use token.text to compare string vs string           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "# Parts of speech tagging\n",
    "# \"en_core_web_sm\" --> pipeline package\n",
    "# Pipeline package \n",
    "# -> binary weights to make predictions , -> vocab , -> meta info and config file \n",
    "nlp = spacy.load(\"en_core_web_sm\") # small english pipeline package from spacy\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text , token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text , token.pos_ , token.dep_ , token.head.text)\n",
    "\n",
    "# dep --> predicted dependency label (like subject , root , object , determiner etc)\n",
    "# head --> parent token this word is attached to.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "ents = doc.ents # this ents methods access the named entities predicted by the ner model , and is applied to the sentence given to it , it have those tokens which are ner in the text.\n",
    "\n",
    "for ent in ents:\n",
    "    print(ent.text , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('determiner', 'Companies, agencies, institutions, etc.', 'Countries, cities, states')\n"
     ]
    }
   ],
   "source": [
    "print(f\"{spacy.explain(\"det\") , spacy.explain(\"ORG\") , spacy.explain(\"GPE\")}\") # helper function to understand tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's official: Apple is the first U.S. public company to reach $1 trillion market value\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It's official: Apple is the first U.S. public company to reach $1 trillion market value\"\n",
    "\n",
    "doc = nlp (text)\n",
    "\n",
    "print(doc.text)\n",
    "print(type(doc)) # <class 'spacy.tokens.doc.Doc'>\n",
    "print(type(doc.text)) # <class 'str'> \n",
    "# always use .text to get strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It', 'PRON', 'nsubj', \"'s\", 'pronoun', 'nominal subject')\n",
      "(\"'s\", 'AUX', 'ccomp', 'is', 'auxiliary', 'clausal complement')\n",
      "('official', 'ADJ', 'acomp', \"'s\", 'adjective', 'adjectival complement')\n",
      "(':', 'PUNCT', 'punct', \"'s\", 'punctuation', 'punctuation')\n",
      "('Apple', 'PROPN', 'nsubj', 'is', 'proper noun', 'nominal subject')\n",
      "('is', 'AUX', 'ROOT', 'is', 'auxiliary', 'root')\n",
      "('the', 'DET', 'det', 'company', 'determiner', 'determiner')\n",
      "('first', 'ADJ', 'amod', 'company', 'adjective', 'adjectival modifier')\n",
      "('U.S.', 'PROPN', 'nmod', 'company', 'proper noun', 'modifier of nominal')\n",
      "('public', 'ADJ', 'amod', 'company', 'adjective', 'adjectival modifier')\n",
      "('company', 'NOUN', 'attr', 'is', 'noun', 'attribute')\n",
      "('to', 'PART', 'aux', 'reach', 'particle', 'auxiliary')\n",
      "('reach', 'VERB', 'relcl', 'company', 'verb', 'relative clause modifier')\n",
      "('$', 'SYM', 'quantmod', 'trillion', 'symbol', 'modifier of quantifier')\n",
      "('1', 'NUM', 'compound', 'trillion', 'numeral', 'compound')\n",
      "('trillion', 'NUM', 'nummod', 'value', 'numeral', 'numeric modifier')\n",
      "('market', 'NOUN', 'compound', 'value', 'noun', 'compound')\n",
      "('value', 'NOUN', 'dobj', 'reach', 'noun', 'direct object')\n"
     ]
    }
   ],
   "source": [
    "# You’ll now get to try one of spaCy’s trained pipeline packages and see its predictions in action. Feel free to try it out on your own text! To find out what a tag or label means, you can call spacy.explain in the loop. For example: spacy.explain(\"PROPN\") or spacy.explain(\"GPE\").\n",
    "\n",
    "# Part 1\n",
    "# Process the text with the nlp object and create a doc.\n",
    "# For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label).\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text , token.pos_ , token.dep_ , token.head.text , spacy.explain(token.pos_) , spacy.explain(token.dep_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text , ent.label_) # iPhone X , model didn't extract it - manually have to correct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env-3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
