Embeddings are two types -> Frequency based (Bag of words , Glove , TF-IDF) and Prediction based (Word2Vec , deep embeddings).

Why Word2Vec preffered over frequency based embeddings?
1. They represent the semantic meaning which frequency based embeddings can't as they only rely on the words count in a document or sentence.
2. The vector embedding size is less than compared to in freqeuency based methods as in freqeuncy based emthods a senytence repreeantion is the whole length of vocab and here it's relatively less and that's a big plus which will make computation faster 
3. The embeddings in frquency based methiods have sparcity in them , and the word2vec are deep embeddings dense vectors which better captures the essence p of words and sentences.

